{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79ee4c9-6472-45e8-b2ec-a862c2fa3c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n|     Date1|     Date2|GreatestDate|\n+----------+----------+------------+\n|2023-02-01|2023-09-01|  2023-09-01|\n|      null|2023-12-01|  2023-12-01|\n|2023-03-01|      null|  2023-03-01|\n+----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import greatest, col\n",
    "from pyspark.sql import functions as F\n",
    "data = [\n",
    "    ('01-Feb-23', '01-Sep-23'),\n",
    "    (' ', '01-Dec-23'),\n",
    "    ('01-Mar-23', ' ')\n",
    "]\n",
    "columns = [\"Date1\",\"Date2\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"Date1\", F.to_date(\"Date1\", \"dd-MMM-yy\")) \\\n",
    "       .withColumn(\"Date2\", F.to_date(\"Date2\", \"dd-MMM-yy\"))\n",
    "\n",
    "df = df.withColumn(\"GreatestDate\", greatest(col(\"Date1\"), col(\"Date2\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913d7fe5-cfd3-4c64-b072-94815a88c33b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id|count|\n+-------+-----+\n|      1|    5|\n|      2|    1|\n|      3|    1|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"Top5CustomersWithHighestClicks\").getOrCreate()\n",
    "data_tuples = [\n",
    "    (1, '2023-01-01 12:00:00', 'click'),\n",
    "    (2, '2023-01-01 12:05:00', 'view'),\n",
    "    (1, '2023-01-01 12:10:00', 'click'),\n",
    "    (3, '2023-01-01 12:15:00', 'view'),\n",
    "    (2, '2023-01-01 12:20:00', 'click'),\n",
    "    (1, '2023-01-01 12:25:00', 'view'),\n",
    "    (3, '2023-01-01 12:30:00', 'click'),\n",
    "    (2, '2023-01-01 12:35:00', 'view'),\n",
    "    (1, '2023-01-01 12:40:00', 'click'),\n",
    "    (3, '2023-01-01 12:45:00', 'view'),\n",
    "    (1, '2023-01-02 12:10:00', 'click'),\n",
    "    (1, '2023-01-03 12:10:00', 'click'),\n",
    "    (1, '2023-01-04 12:10:00', 'view'),\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"timestamp\", \"interaction_type\"]\n",
    "df = spark.createDataFrame(data_tuples, columns)\n",
    "\n",
    "clicks_df = df.filter(col(\"interaction_type\") == \"click\")\n",
    "clicks_count_df = clicks_df.groupBy(\"user_id\").count()\n",
    "\n",
    "sorted_clicks_count_df = clicks_count_df.orderBy(col(\"count\").desc())\n",
    "\n",
    "top_5_customers_with_highest_clicks = sorted_clicks_count_df.limit(5)\n",
    "top_5_customers_with_highest_clicks.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65f447c-db51-4978-8b81-f985f967236c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n|user_id|click_count|\n+-------+-----------+\n|      1|          5|\n|      2|          1|\n|      3|          1|\n+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "spark = SparkSession.builder.appName(\"TopClick\").getOrCreate()\n",
    "data = [\n",
    "    (1, '2023-01-01 12:00:00', 'click'),\n",
    "    (2, '2023-01-01 12:05:00', 'view'),\n",
    "    (1, '2023-01-01 12:10:00', 'click'),\n",
    "    (3, '2023-01-01 12:15:00', 'view'),\n",
    "    (2, '2023-01-01 12:20:00', 'click'),\n",
    "    (1, '2023-01-01 12:25:00', 'view'),\n",
    "    (3, '2023-01-01 12:30:00', 'click'),\n",
    "    (2, '2023-01-01 12:35:00', 'view'),\n",
    "    (1, '2023-01-01 12:40:00', 'click'),\n",
    "    (3, '2023-01-01 12:45:00', 'view'),\n",
    "    (1, '2023-01-02 12:10:00', 'click'),\n",
    "    (1, '2023-01-03 12:10:00', 'click'),\n",
    "    (1, '2023-01-04 12:10:00', 'view'),\n",
    "]\n",
    "columns = ['user_id', 'timestamp', 'interaction_type']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df_filtered = df.where(col('interaction_type') == 'click')\n",
    "df_click_counts = df_filtered.groupBy('user_id').agg(count(col('interaction_type')).alias('click_count')).sort(col('click_count').desc())\n",
    "top_5_clickers = df_click_counts.limit(5)\n",
    "top_5_clickers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c518d9-b609-492a-9d44-1182f2936149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|missingids|\n+----------+\n|         4|\n|         5|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#find the missing numbers in the column \"id\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"FindMissingNumber\").getOrCreate()\n",
    "data = [\n",
    "    (1,),\n",
    "    (2,),\n",
    "    (3,),\n",
    "    (6,),\n",
    "    (7,),\n",
    "    (8,),\n",
    "]\n",
    "df = spark.createDataFrame(data).toDF(\"id\")\n",
    "expected_id_range = spark.range(1, df.count() + 1)  \n",
    "miss_ids = expected_id_range.select(col(\"id\").cast(df.schema[\"id\"].dataType)).exceptAll(df)\n",
    "miss_ids = missing_ids.withColumnRenamed(\"id\", \"missingids\")\n",
    "miss_ids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd099a2-0da4-418e-b62c-8330f6a30e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter the number of elements:  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter element 1:  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter element 2:  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter element 3:  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter element 4:  4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even numbers: [2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Take input from user to create a list n displsy display even numbers from it \n",
    "n = int(input(\"Enter the number of elements: \"))\n",
    "user_list = []\n",
    "for i in range(n):\n",
    "    user_list.append(int(input(\"Enter element {}: \".format(i + 1))))\n",
    "even_numbers = [x for x in user_list if x % 2 == 0]\n",
    "print(\"Even numbers:\", even_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f11a632-281d-4189-b8e5-86f2c6fe130b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Value</th></tr></thead><tbody><tr><td>2</td></tr><tr><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2
        ],
        [
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Value",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Create a table to store user input\n",
    "CREATE TABLE UserInput (\n",
    "    Value INT\n",
    ");\n",
    "\n",
    "-- Insert user input into the table\n",
    "INSERT INTO UserInput (Value) VALUES (1), (2), (3), (4), (5);\n",
    "\n",
    "-- Display only even numbers from the table\n",
    "SELECT Value\n",
    "FROM UserInput\n",
    "WHERE Value % 2 = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a10360-333b-419e-ba86-f5cc6e9989e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    2|\n|    4|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SampleEvenNumbers\").getOrCreate()\n",
    "data = [(1,), (2,), (3,), (4,), (5,)]\n",
    "df = spark.createDataFrame(data, [\"value\"])\n",
    "even_numbers_df = df.filter(col(\"value\") % 2 == 0)\n",
    "even_numbers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea756ede-9ff9-4135-abd8-f319b5c35a70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|value|\n+-----+\n|    2|\n|    4|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"SampleEvenNumbers\").getOrCreate()\n",
    "data = [(1,), (2,), (3,), (4,), (5,)]\n",
    "df = spark.createDataFrame(data, [\"value\"])\n",
    "even_numbers_df = df.filter(col(\"value\") % 2 == 0)\n",
    "even_numbers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7de7278-c25a-4593-9483-975307db1ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#like and not like operatorin pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"LikeNotLikeExample\").getOrCreate()\n",
    "df=spark.read.csv(\"dbfs:/FileStore/employees.csv\",header=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53146de-e94a-4517-af44-8abf4d7ecff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST_NAME starting with 'J':\n+-----------+-----------+---------+-------+------------+---------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID| FIRST_NAME|LAST_NAME|  EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+-----------+---------+-------+------------+---------+----------+------+--------------+----------+-------------+\n|        200|   Jennifer|   Whalen|JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n|        110|       John|     Chen|  JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|\n|        112|Jose Manuel|    Urman|JMURMAN|515.124.4469|07-MAR-06|FI_ACCOUNT|  7800|            - |       108|          100|\n|        125|      Julia|    Nayer| JNAYER|650.124.1214|16-JUL-05|  ST_CLERK|  3200|            - |       120|           50|\n|        127|      James|   Landry|JLANDRY|650.124.1334|14-JAN-07|  ST_CLERK|  2400|            - |       120|           50|\n|        131|      James|   Marlow|JAMRLOW|650.124.7234|16-FEB-05|  ST_CLERK|  2500|            - |       121|           50|\n|        133|      Jason|   Mallin|JMALLIN|650.127.1934|14-JUN-04|  ST_CLERK|  3300|            - |       122|           50|\n|        139|       John|      Seo|   JSEO|650.121.2019|12-FEB-06|  ST_CLERK|  2700|            - |       123|           50|\n|        140|     Joshua|    Patel| JPATEL|650.121.1834|06-APR-06|  ST_CLERK|  2500|            - |       123|           50|\n+-----------+-----------+---------+-------+------------+---------+----------+------+--------------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "j_names_df = df.filter(col(\"FIRST_NAME\").like(\"J%\"))\n",
    "print(\"FIRST_NAME starting with 'J':\")\n",
    "j_names_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4126d9-d43a-4abd-bb51-04f3a73fc15a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST_NAME not starting with 'J':\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|\n|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|\n|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|\n|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|\n|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|\n|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|\n|        111|    Ismael|  Sciarra|ISCIARRA|515.124.4369|30-SEP-05|FI_ACCOUNT|  7700|            - |       108|          100|\n|        113|      Luis|     Popp|   LPOPP|515.124.4567|07-DEC-07|FI_ACCOUNT|  6900|            - |       108|          100|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "non_j_names_df = df.filter(~col(\"FIRST_NAME\").like(\"J%\"))\n",
    "print(\"FIRST_NAME not starting with 'J':\")\n",
    "non_j_names_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3350944695723339,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark coding questions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
